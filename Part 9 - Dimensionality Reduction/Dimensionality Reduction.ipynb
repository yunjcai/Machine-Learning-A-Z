{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of Dimensionality Reduction techniques:\n",
    "1. Feature Selection: (see [Part 2 - Regression])    \n",
    "    * Backward Elimination\n",
    "    * Forward Selection\n",
    "    * Bidirectional Elimination\n",
    "    * Score Comparison\n",
    "2. Feature Extraction: \n",
    "    * Principal Component Analysis (PCA)\n",
    "    * Linear Discriminant Analysis (LDA)\n",
    "    * Kernel PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is considered to be one of the most used Unsupervised algorithms (most popular one to use). From the $m$ independent variables of the dataset, PCA extracts $p≤m$ new independent variables that explain the most the variance of the dataset, regardless of the dependent variable.\n",
    "\n",
    "The goal of PCA is to:\n",
    "* Identify patterns in data\n",
    "* Detect the correlation between variables (To reduce the dimensionality for the strong correlation)\n",
    "\n",
    "Reduce the dimensions of a $d$-dimensional dataset by projecting it onto a ($k$)-dimensional subspace (where $k<d$)\n",
    "\n",
    "##### [A Summary of the PCA Approach](https://plot.ly/ipython-notebooks/principal-component-analysis/):\n",
    "* Standardize the data.\n",
    "* Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
    "* Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where k is the number of dimensions of the new feature subspace ($k≤d$)/.\n",
    "* Construct the projection matrix $W$ from the selected $k$ eigenvectors.\n",
    "* Transform the original dataset $X$ via $W$ to obtain a $k$-dimensional feature subspace $Y$.\n",
    "\n",
    "[Visualization of PCA Approach](http://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "* PCA learns about the relationship between $X$ and $Y$ values\n",
    "* Find list of principal axes\n",
    "\n",
    "##### [如何通俗易懂地讲解什么是 PCA 主成分分析？](https://www.zhihu.com/question/41120789)\n",
    "主元分析也就是PCA，主要用于数据降维。\n",
    "\n",
    "**什么是降维？**\n",
    "采集了房屋的价格和面积，可以看出两者完全正相关，有一列其实是多余的：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_1.JPG?raw=true' width='300'><br>\n",
    "$\\overline{X}$是均值，以$\\overline{X}$为原点($0$)，那么上面表格中数字全部减去均值：(中心化)<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_2.JPG?raw=true' width='300'><br>\n",
    "把这个二维数据画在坐标轴上，横纵坐标分别为“房价”、“面积”，可以看出它们排列为一条直线：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_3.JPG?raw=true' width='200'><br>\n",
    "旋转后的坐标系，横纵坐标不再代表“房价”、“面积”了，而是两者的混合（术语是线性组合），这里把它们称作“主元1”、“主元2”，坐标值很容易用勾股定理计算出来，比如$a$在“主元1”的坐标值为：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_4.JPG?raw=true' width='200'><br>\n",
    "很显然$a$在“主元2”上的坐标为$0$，把所有的数字换算到新的坐标系上：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_5.JPG?raw=true' width='200'><br>\n",
    "因为“主元2”全都为0，完全是多余的，我们只需要“主元1”就够了，这样就又把数据降为了一维，而且没有丢失任何信息。\n",
    "\n",
    "**非理想情况如何降维？**\n",
    "现实中虽然正比，但总会有些出入，虽然数据看起来很接近一条直线，但是终究不在一条直线上：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_6.JPG?raw=true' width='500'><br>\n",
    "\n",
    "**主元分析（PCA）**\n",
    "按列解读得到两个向量：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_7.JPG?raw=true' width='200'><br>\n",
    "组成协方差矩阵：$Q = \\frac{1}{n}P = \\left(\\begin{array}{rr}Var(X) & Cov(X,Y) \\\\Cov(X,Y) & Var(Y) \\\\\\end{array}\\right) = \\frac{1}{5}\\left(\\begin{array}{rr}X\\cdot X & X\\cdot Y \\\\X\\cdot Y & Y\\cdot Y \\\\\\end{array}\\right)= \\frac{1}{5}\\left(\\begin{array}{rr}57.2 & 45.2 \\\\45.2 & 36.7 \\\\\\end{array}\\right)\n",
    "$<br>\n",
    "进行奇异值分解：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_9.JPG?raw=true' width='200'>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_11.JPG?raw=true' width='400'><br>\n",
    "根据之前的分析，主元1应该匹配最大奇异值对应的奇异向量，主元2匹配最小奇异值对应的奇异向量，即：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_12.JPG?raw=true' width='200'><br>\n",
    "以这两个为主元画出来的坐标系就是这样的：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_13.JPG?raw=true' width='200'><br>\n",
    "如下算出新坐标，比如对于$a$ ：<br>\n",
    "$X_1 = a\\cdot e_1 = -6.94$<br>\n",
    "$X_2 = a\\cdot e_2 = 0.084$<br>\n",
    "以此类推，得到新的数据表：<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%209%20-%20Dimensionality%20Reduction/pca_14.JPG?raw=true' width='200'><br>\n",
    "主元2整体来看，数值很小，丢掉损失的信息也非常少，这样就实现了非理想情况下的降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Wine.csv')\n",
    "X = dataset.iloc[:,:13].values\n",
    "y = dataset.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling must be applied to the Dimension Reduction Techniques like PCA or LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_components: The number of extracted features we want to get. Since we don't know how many principal components. Therefore, we put 'None' here first. We need the Explained Variance Vector to see how many principal components we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=None)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need have a look at the cumulative explained variance of the different principal components, so we need create the explained variance vector which contains the % of variance explained by each of the principal components that we extracted here (from the most variance to least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36198848, 0.1920749 , 0.11123631, 0.0706903 , 0.06563294,\n",
       "       0.04935823, 0.04238679, 0.02680749, 0.02222153, 0.01930019,\n",
       "       0.01736836, 0.01298233, 0.00795215])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case, we take only one principal component that will explain 36.7% of the variance. Then if we take two principal components that will explain 36.7% + 19.2% = 56% of the variance. In our example, we only need 2 principal variables, so we take 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X) # Now X only has 2 independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_stats=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "# Predict the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "244.545px",
    "left": "973.273px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
