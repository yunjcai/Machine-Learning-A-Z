{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection And Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we know which machine learning model is the best one to solve the problem?<br>\n",
    "How can we tell whether the problem is \"regrssion\", \"classification\" or \"clustering\" problem?**\n",
    "<font color='blue'>\n",
    "* Take look at the dependent variables:\n",
    "    * Clustering: if No dependent variables.\n",
    "    * Regression: if the dependent variable is continuous outcome\n",
    "    * Classification: if the dependent variable is categorical outcome</font>\n",
    "    \n",
    "**Is my problem a linear or nonlinear problem?**<br>\n",
    "Grid Search will tell us if we should choose linear model (like SVM) or nonlinear model (like kernel SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation is a very efficient way to evaluate the model performance.\n",
    "\n",
    "We will get a very different accuracy on the test set, when we run the model and test again on the different test set. Therefore, judging the model performance only on one accuracy on one test set is not super relevant.<br>\n",
    "\n",
    "K-Fold Cross Validation: Spinning the training set into ten fold (most time K = 10). And we train the model with nine fold and test it on the last remaining fold. Since we have 10 folds, then we can make 10 different combinations of 9 folds to train the model and 1 fold to test it.<br>\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%2010%20-%20Model%20Selection%20And%20Boosting/ms1.JPG?raw=true' width='500'><br>\n",
    "That means we can train and test the model on 10 combinations of the training and test sets. Then we can take an average of different accuracy up to 10 evaluations and compute the standard deviation to have look at the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=0,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the test results and Confusion matrix is the first way of evaluating the model.<br>\n",
    "But NOT the best way!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64,  4],\n",
       "       [ 3, 29]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_val_score will return 10 accuracy for each one of the 10 combinations that will be created through K-Fold Cross Validation.<br>\n",
    "\n",
    "Each combination is composed of 9 folds to train and 1 fold to test.<br>\n",
    "\n",
    "##### cross_val_score arguments:\n",
    "estimator: it's the model which is the classifier in this example.<br>\n",
    "X: The data to fit (training set).<br>\n",
    "y: The dependent variable to try to predict (training set).<br>\n",
    "cv: The number of folds we want to split the training set into. The most common choice for the cv number is 10.\n",
    "n_jobs: For very large dataset, set it = -1, means 'all CPUs'. Then you can run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80645161, 0.96666667, 0.8       , 0.93333333, 0.86666667,\n",
       "       0.8       , 0.93333333, 0.93333333, 0.96666667, 0.96551724])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a vector that will take the 10 accuracy \n",
    "# through the 10 combinations created by K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train,\n",
    "                             cv = 10)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means when we test the performance of the model on the different 1 test set, we will get different accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8971968854282535\n",
      "0.0680430514209002\n"
     ]
    }
   ],
   "source": [
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search is the technique to improve model performance by finding the optimal values of the hyper parameters. It helps us to know which parameter to select and what's its optimal value when we make a machine learning model\n",
    "\n",
    "Any Machine Learning model is composed of 2 types of parameters:\n",
    "* Parameters are learned through the machine learning\n",
    "* Hyper parameters: Parameters we choosed, ie. kernel in SVM model, penalty paramaters...\n",
    "\n",
    "Normally, Grid Search can be applied after fitting the model to the training set. Because Grid Search needs the trained machine learning as input.\n",
    "\n",
    "注：In the following example, we evaluate the model without Grid Search first. Then do the Grid Search to see the difference. So adding Grid Search section after K-Fold Cross Validation section (直接承接上一章)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the different parameters of which we want to find the optimal values.\n",
    "\n",
    "For example, we used 'kernel'='rbf' and 'C' = 1 (default) in SVC class.<br>\n",
    "* C: The parameter for regularization to prevent overfitting. Increasing C the more it will prevent overfitting, so don't increase too much otherwise, it will cause underfitting problem.\n",
    "* gamma: For nonlinear only, Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "\n",
    "And we will include both 'kernel' and 'C' and 'gamma' in the Grid Search model. <br>\n",
    "Grid Search will tell the kernel should be linear or nonlinear, what's the best value of 'C' and 'gamma'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of dictionaries, so the key identifiers in the dictionary \n",
    "# will be the parameters that we want to optimize, and each of these key identifiers\n",
    "# we will give it several values to be tested by the Grid Search Model.\n",
    "# And among these values the grid search model will find the best one.\n",
    "\n",
    "# The first option (first dictionary) of grid search will investigate\n",
    "# whether it's linear model and what's the best value of 'C'\n",
    "\n",
    "# The second option (second dictionary) of grid search is nonlinear option\n",
    "# so we will set 'gamma'(nonlinear only) in this option\n",
    "# since 'gamma' default is 'auto' which is 1/n_features, and we have 2 features\n",
    "# so the range in the example will be [0.001 - 0.5]\n",
    "parameters = [{'C':[1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "              {'C':[1, 10, 100, 1000], 'kernel': ['rbf'], \n",
    "               'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search selects the best parameters based on one performance metric (scoring)\n",
    "# it can be 'accuracy' or 'precision' or 'recall'...\n",
    "# The comment practice is useing 'accuracy'\n",
    "\n",
    "# cv is the K-Fold Cross Validation. It means Grid Search will evaluate \n",
    "# the performance of each model with their own set of parameters.\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10, # 10 Fold Cross Validation will be used\n",
    "                           n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033333333333333\n",
      "{'C': 1, 'gamma': 0.4, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Once we create grid search object, then we can train it.\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best accuracy score with the best selections of the parameter list we want to try\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(best_accuracy)\n",
    "# best parameters have been selected by grid search\n",
    "best_parameters = grid_search.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a gradient boosting with trees. The most powerful tool for the Machine Learning!!!\n",
    "\n",
    "<font color = 'blue'>**It can be used on both Regression and Classification:**\n",
    "\n",
    "* from xgboost import XGBRegressor     OR     _xgboost.XGBRegressor_\n",
    "* from xgboost import XGBClassifier     OR     _xgboost.XGBClassifier_\n",
    "</font>\n",
    "Recommend to use XGBoost to work on a large dataset!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "# Independent variables: CreditScore,Geography,Gender,Age,\n",
    "# Tenure,Balance,NumOfProducts,HasCrCard,IsActiveMember,EstimatedSalary\n",
    "# ANN will determine which independent variable will be more important.\n",
    "X = dataset.iloc[:,3:13].values\n",
    "y = dataset.loc[:,'Exited'].values\n",
    "\n",
    "# Encodes any categorical data in the dataset\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# In the dataset, only 'Geography' and 'Gender' need to be encoded.\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:,1] = labelencoder_X_1.fit_transform(X[:,1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:,2] = labelencoder_X_2.fit_transform(X[:,2])\n",
    "\n",
    "# The categorical variables are NOT ordinal \n",
    "# (No relational order between the categorical variables)\n",
    "# For example, France (2) is NOT higher than Germany (1)\n",
    "# So we need create dummy variables for these categorical variables\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "# In order to convert X to be a matrix, we need add '.toarray()' in the end.\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "# To remove one dummy variable in order to avoid falling into the dummy variable trap.\n",
    "X = X[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>All Deep Learning MUST have feature scaling.</font> \n",
    "\n",
    "But XGBoost is based on the models of decision trees. So the feature scaling is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier Arguments:\n",
    "* learning_rate: We had in deep learning\n",
    "* n_estimators: the number of trees\n",
    "\n",
    "In the example, it will be the most simple case -- Use all default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting XGBoost to the Training set\n",
    "from xgboost import XGBClassifier as XGBC\n",
    "classifier = XGBC().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting And Evaulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1521   74]\n",
      " [ 197  208]]\n",
      "0.8629994451163204\n",
      "0.010677872171663988\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Applying K-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, \n",
    "                             y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "301.545px",
    "left": "411.818px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
