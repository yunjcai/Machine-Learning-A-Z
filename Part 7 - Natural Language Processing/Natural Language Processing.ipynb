{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing(自然语言处理)，或简称为 NLP，是 AI 的子领域，重点放在使计算机能够理解和处理人类语言。<br>\n",
    "计算机更擅长理解结构化数据(如电子表格和数据库表)，让计算机去理解人类语言(text and language)，那就是——把人类语言（尽可能）结构化。<br>\n",
    "\n",
    "Teaching machines to understand what is said in spoken and written word is the focus of Natural Language Processing. \n",
    "\n",
    "For example:\n",
    "* Whenever you dictate something into your iPhone / Android device that is then converted to text, that’s an NLP algorithm in action.\n",
    "* You can also use NLP on a text review to predict if the review is a good one or a bad one. \n",
    "* You can use NLP on an article to predict some categories of the articles you are trying to segment. \n",
    "* You can use NLP on a book to predict the genre of the book. \n",
    "* And it can go further, you can use NLP to build a machine translator or a speech recognition system (you use classification algorithms to classify language. Speaking of classification algorithms)\n",
    "\n",
    "Most of NLP algorithms are classification models, and they include Logistic Regression, Naive Bayes, CART which is a model based on decision trees, Maximum Entropy again related to Decision Trees, Hidden Markov Models which are models based on Markov processes.\n",
    "\n",
    "A very well-known model in NLP is the Bag of Words model. It is a model used to preprocess the texts to classify before fitting the classification algorithms on the observations containing the texts.<br>\n",
    "\n",
    "Main NLP library examples:\n",
    "* Natural Language Toolkit - NLTK (very powerful)\n",
    "* SpaCy\n",
    "* Stanford NLP\n",
    "* OpenNLP\n",
    "\n",
    "##### 参考：\n",
    "在机器学习中做任何复杂的事情通常意味着需要建立一条流水线 (pipeline)。这个想法是把你的问题分解成非常小的部分，然后用机器学习来分别解决每个部分，最后通过把几个互相馈送结果的机器学习模型连接起来，就可以解决非常复杂的问题。[自然语言处理是如何工作的？一步步教你构建 NLP 流水线](https://www.jiqizhixin.com/articles/081203)\n",
    "\n",
    "GitHub上还有一套NLP课程。[GitHub NLP Course](https://github.com/yandexdataschool/nlp_course)<br>课程为期13周，从文本嵌入分类开始，讲到Seq2Seq，再到机器翻译、对话系统、对抗学习等等，内容丰富。入门选手可以考虑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .tsv file, because no one usually uses 'tab' in review. So it can parse text easily by using tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas will know the two columns are separated by 'tab'\n",
    "# To avoid double quotes \"\" issue (quoting = 3 to ignore \"double quotes\")\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t',quoting=3)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Texts to Prepare Them for the Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Only keep letters (A-Z) in the review and remove the numbers(unless the numbers can have a significant impact), punctuations(ie.'...'). Removed letters will be replaced by ' '(space).<br>\n",
    "**STEP 2:** Putting all the letters in lowercase.<br>\n",
    "**STEP 3:** Remove the non-significant words ('the','that','and'...). Keep the words which are relevant to predict the positive or negative reviews.<br>\n",
    "**STEP 4:** Stemming (taking the root of the words. ie. love,loved,loves). In order not to have too many words in the end.<br>\n",
    "**STEP 5:** Transform list back to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the 1st review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Only keep letters (A-Z) in the review and remove the numbers(unless the numbers can have a significant impact), punctuations(ie.'...'). Removed letters will be replaced by ' '(space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow    Loved this place '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regular Expression 正则表达式\n",
    "import re\n",
    "\n",
    "# [^] is what we want to keep\n",
    "review = re.sub('[^a-zA-Z]',' ',dataset['Review'][0])\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:** Putting all the letters in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow    loved this place '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = review.lower()\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:** Remove the non-significant words ('the','that','and'...). Keep the words which are relevant to predict the positive or negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **stop words list** contains all the words that are generically irrelevant in any text to predict the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Cai's\n",
      "[nltk_data]     family\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# use download function in nltk library\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4:** Stemming (taking the root of the words. ie. love,loved,loves). In order not to have too many words in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow', 'love', 'place']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split a string into each single word in a list\n",
    "review = review.split()\n",
    "\n",
    "# Create Stemming object\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# go through all the words in a review, \n",
    "# and remove the words which are in the stop words list\n",
    "# stopwords has a lot of list in different language, we only focus on english.\n",
    "# set will be way faster to go through all the different words than list \n",
    "# (recommend to cast list to set to speed up)\n",
    "# apply stem function onto each single word\n",
    "review = [ps.stem(word) for word in review \n",
    "          if not word in set(stopwords.words('english'))]\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5:** Transform list back to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow love place'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use join function to combine all words together to obtain a string\n",
    "# but we still need separate each word by something, \n",
    "# otherwise all words will stick together (ie. Iloveyou)\n",
    "review = ' '.join(review) # join all words together and separated by the space\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the new list of the 1000 clean reviews\n",
    "# in NLP, a corpus is a common word and a collection of text at the same type\n",
    "# (articles, books, web, html pages...)\n",
    "corpus = []\n",
    "\n",
    "for i in range(0,dataset.shape[0]):\n",
    "    review = re.sub('[^a-zA-Z]',' ',dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review \n",
    "              if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of Words Model\n",
    "<img src='https://github.com/yunjcai/Machine-Learning-A-Z/blob/master/Part%207%20-%20Natural%20Language%20Processing/bag_of_words_1.JPG?raw=true' width='400'><br>\n",
    "To create a Bag of Words model is: \n",
    "* Take all the different but unique words of the cleaned text\n",
    "* Create one column for each word\n",
    "* Put the column into a table, the rows will be each item in corpus, each cell will be the number of word (column) occur in row. \n",
    "\n",
    "In the case, the table (matrix) will contain a lot of zeros, which is called a sparse matrix (bag of words model). The fact that we have a lot of zeros is called sparsity.<br>\n",
    "We try to reduce the sparsity as much as possible when we work with the machine learning model.\n",
    "\n",
    "In the bag of words, each column can be corresponding to one specific word is one independent variable itself.\n",
    "\n",
    "We will use the bag of words model to predict the categorical result (ie. positive or negative) in the end. Now, it becomes the Classification machine learning model, so it needs to train the model with those cleaned words and will find the correlation between words and results by itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CountVectorizer Class to create the huge sparse matrix\n",
    "\n",
    "CountVectorizer Class also has some input parameters to clean the text:\n",
    "* lowercase - (True, default) Convert all characters to lowercase before tokenizing.\n",
    "* token_pattern - Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "* stop_words - Remove the non-significant words ('the','that','and'...). Keep the words which are relevant to predict the positive or negative reviews.\n",
    "\n",
    "In the case, we can clean the text directly in the Class CountVectorizer by applying with these parameters.\n",
    "\n",
    "However, cleaning text manually gives us more control and options and give the possibility to clean fully the text as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 可以使用在 CoutVectorizer() 中的 'max_features' 变量\n",
    "# keep the most frequent words and remove the non-relevant words \n",
    "# that appear only in one or two times\n",
    "cv = CountVectorizer(max_features=1500) #保留最常用的1500个words\n",
    "# To create the huge sparse matrix\n",
    "# since we need the matrix of features, in order to get the matrix\n",
    "# we need add .toarray() in the end\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# Create Dependent variables in order to train the model\n",
    "# 为了得到 nparray, 需要用 .values\n",
    "y = dataset['Liked'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Machine Learning Models onto the Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为是通过 review 中的 words 来推测 positive or negative。所以是 Classification 问题。<br>\n",
    "\n",
    "**在 NLP 中，最常用的 Classification Model 是 Naive Bayes, Decision Tree 或者 Random Forest classification.**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need feature scaling \n",
    "# because the sparse matrix has mostly zeros and a few ones\n",
    "# and a very few twos or threes\n",
    "# so feature scaling is unnecessary.\n",
    "\n",
    "## Feature Scaling\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X = sc.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate the model performance\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n",
      "Accuracy =  0.73\n",
      "Prcision =  0.6842105263157895\n",
      "Recall =  0.883495145631068\n",
      "F1 Score =  0.722465894997933\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.57      0.67        97\n",
      "           1       0.68      0.88      0.77       103\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.75      0.73      0.72       200\n",
      "weighted avg       0.75      0.73      0.72       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB().fit(X_train,y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_nb = confusion_matrix(y_test,y_pred)\n",
    "print(cm_nb)\n",
    "\n",
    "print('Accuracy = ', accuracy_score(y_test,y_pred))\n",
    "print('Prcision = ', precision_score(y_test,y_pred))\n",
    "print('Recall = ', recall_score(y_test,y_pred))\n",
    "print('F1 Score = ', f1_score(y_test,y_pred,average='weighted'))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74 23]\n",
      " [35 68]]\n",
      "Accuracy =  0.71\n",
      "Prcision =  0.7472527472527473\n",
      "Recall =  0.6601941747572816\n",
      "F1 Score =  0.7094775297767992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72        97\n",
      "           1       0.75      0.66      0.70       103\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.71      0.71      0.71       200\n",
      "weighted avg       0.71      0.71      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting Decision Tree to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion='entropy',random_state=0).fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm_dt = confusion_matrix(y_test,y_pred)\n",
    "print(cm_dt)\n",
    "\n",
    "print('Accuracy = ', accuracy_score(y_test,y_pred))\n",
    "print('Prcision = ', precision_score(y_test,y_pred))\n",
    "print('Recall = ', recall_score(y_test,y_pred))\n",
    "print('F1 Score = ', f1_score(y_test,y_pred,average='weighted'))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87 10]\n",
      " [46 57]]\n",
      "Accuracy =  0.72\n",
      "Prcision =  0.8507462686567164\n",
      "Recall =  0.5533980582524272\n",
      "F1 Score =  0.7122659846547316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.90      0.76        97\n",
      "           1       0.85      0.55      0.67       103\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.75      0.73      0.71       200\n",
      "weighted avg       0.76      0.72      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "# n_estimators: 将使用多少 decision trees 来组成 random forest (default = 10)\n",
    "classifier = RFC(n_estimators=10,criterion='entropy',random_state=0).fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm_rf = confusion_matrix(y_test,y_pred)\n",
    "print(cm_rf)\n",
    "\n",
    "print('Accuracy = ', accuracy_score(y_test,y_pred))\n",
    "print('Prcision = ', precision_score(y_test,y_pred))\n",
    "print('Recall = ', recall_score(y_test,y_pred))\n",
    "print('F1 Score = ', f1_score(y_test,y_pred,average='weighted'))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not enough, so we should also look at other performance metrics like \n",
    "* Precision (measuring exactness)\n",
    "* Recall (measuring completeness) \n",
    "* F1 Score (compromise between Precision and Recall)\n",
    "\n",
    "Please find below these metrics formulas (TP = # True Positives, TN = # True Negatives, FP = # False Positives, FN = # False Negatives):\n",
    "* $Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "* $Precision = \\frac{TP}{TP + FP}$\n",
    "* $Recall = \\frac{TP}{TP + FN}$\n",
    "* $F1 Score = 2 \\frac{Precision * Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(cm): # cm is confusion_matrix\n",
    "    TP = cm[0,0]\n",
    "    TN = cm[1,1]\n",
    "    FP = cm[0,1]\n",
    "    FN = cm[1,0]\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    print('Accuracy = ', accuracy)\n",
    "    print('Prcision = ', precision)\n",
    "    print('Recall = ', recall)\n",
    "    print('F1 Score = ', F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.73\n",
      "Prcision =  0.5670103092783505\n",
      "Recall =  0.8208955223880597\n",
      "F1 Score =  0.6707317073170731\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(cm_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.71\n",
      "Prcision =  0.7628865979381443\n",
      "Recall =  0.6788990825688074\n",
      "F1 Score =  0.7184466019417477\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(cm_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.72\n",
      "Prcision =  0.8969072164948454\n",
      "Recall =  0.6541353383458647\n",
      "F1 Score =  0.7565217391304349\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(cm_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "252.4px",
    "left": "214px",
    "right": "20px",
    "top": "114px",
    "width": "445.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
